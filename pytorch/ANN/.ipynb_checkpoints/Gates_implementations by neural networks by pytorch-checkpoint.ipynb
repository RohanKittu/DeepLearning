{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are really two decisions that must be made regarding the hidden layers: how many hidden layers to actually have in the neural network and how many neurons will be in each of these layers. We will first examine how to determine the number of hidden layers to use with the neural network.\n",
    "\n",
    "### The Number of Hidden Layers\n",
    "Problems that require two hidden layers are rarely encountered. However, neural networks with two hidden layers can represent functions with any kind of shape. There is currently no theoretical reason to use neural networks with any more than two hidden layers. In fact, for many practical problems, there is no reason to use any more than one hidden layer. Table 5.1 summarizes the capabilities of neural network architectures with various hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Number of Hidden Layers | Result |\n",
    "\n",
    " 0 - Only capable of representing linear separable functions or decisions.\n",
    "\n",
    " 1 - Can approximate any function that contains a continuous mapping\n",
    "from one finite space to another.\n",
    "\n",
    " 2 - Can represent an arbitrary decision boundary to arbitrary accuracy\n",
    "with rational activation functions and can approximate any smooth\n",
    "mapping to any accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deciding the number of hidden neuron layers is only a small part of the problem. You must also determine how many neurons will be in each of these hidden layers. This process is covered in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Number of Neurons in the Hidden Layers\n",
    "\n",
    "Deciding the number of neurons in the hidden layers is a very important part of deciding your overall neural network architecture. Though these layers do not directly interact with the external environment, they have a tremendous influence on the final output. Both the number of hidden layers and the number of neurons in each of these hidden layers must be carefully considered.\n",
    "\n",
    "Using too few neurons in the hidden layers will result in something called underfitting. Underfitting occurs when there are too few neurons in the hidden layers to adequately detect the signals in a complicated data set.\n",
    "\n",
    "Using too many neurons in the hidden layers can result in several problems. First, too many neurons in the hidden layers may result in overfitting. Overfitting occurs when the neural network has so much information processing capacity that the limited amount of information contained in the training set is not enough to train all of the neurons in the hidden layers. A second problem can occur even when the training data is sufficient. An inordinately large number of neurons in the hidden layers can increase the time it takes to train the network. The amount of training time can increase to the point that it is impossible to adequately train the neural network. Obviously, some compromise must be reached between too many and too few neurons in the hidden layers.\n",
    "\n",
    "There are many rule-of-thumb methods for determining the correct number of neurons to use in the hidden layers, such as the following:\n",
    "\n",
    "The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "The number of hidden neurons should be less than twice the size of the input layer.\n",
    "These three rules provide a starting point for you to consider. Ultimately, the selection of an architecture for your neural network will come down to trial and error. But what exactly is meant by trial and error? You do not want to start throwing random numbers of layers and neurons at your network. To do so would be very time consuming. Chapter 8, “Pruning a Neural Network” will explore various ways to determine an optimal structure for a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "\n",
    "https://arxiv.org/pdf/1707.09725.pdf#page=11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About activation functions in neural network\n",
    "\n",
    "https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6456563062257954\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))\n",
    "\n",
    "print(sigmoid(0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataset\n",
    "#And gate\n",
    "\n",
    "import numpy as np\n",
    "gate=np.array([[0,0,0],[0,1,0],[1,0,0],[1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gate[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = gate[:,0:2]\n",
    "train_y = gate[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the neural network art\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#our class must extend nn.Module\n",
    "class MyClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyClassifier,self).__init__()\n",
    "        #Our network consists of 3 layers. 1 input, 1 hidden and 1 output layer\n",
    "        #This applies Linear transformation to input data. \n",
    "        self.layer1 = nn.Linear(2,3)\n",
    "        self.layer2 = nn.Linear(3,2)\n",
    "    \n",
    "    \n",
    "    #This must be implemented\n",
    "    def forward(self,x):\n",
    "        #of the first layer\n",
    "        x = self.layer1(x)\n",
    "        #Activation function is Relu. Feel free to experiment with this\n",
    "        x = F.tanh(x)\n",
    "        #This produces output\n",
    "        x = self.layer2(x)    \n",
    "        return x\n",
    "    \n",
    "    #This function takes an input and predicts the class, (0 or 1)        \n",
    "    def predict(self,x):\n",
    "        #Apply softmax to output. \n",
    "        prediction = F.sigmoid(self.forward(x)) #F.softmax(self.forward(x))\n",
    "        ans = []\n",
    "        #Pick the class with maximum weight\n",
    "        for t in prediction:\n",
    "            if t[0]>t[1]:\n",
    "                ans.append(0)\n",
    "            else:\n",
    "                ans.append(1)\n",
    "            return torch.tensor(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#Initialize the model        \n",
    "model = MyClassifier()\n",
    "#Define loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the array into pytorch object\n",
    "train_x = torch.from_numpy(train_x).type(torch.FloatTensor)\n",
    "train_y = torch.from_numpy(train_y).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0034602076124567475,\n",
       " 0.11418685121107267,\n",
       " 0.22837370242214533,\n",
       " 1.0,\n",
       " 0.07612456747404844,\n",
       " 0.1903114186851211,\n",
       " 0.2664359861591695,\n",
       " 0.2041522491349481,\n",
       " 0.34256055363321797,\n",
       " 0.020761245674740483,\n",
       " 0.02422145328719723,\n",
       " 0.02768166089965398,\n",
       " 0.031141868512110725]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#min max normilization\n",
    "def normilation(lis):\n",
    "    return [(i-min(lis))/(max(lis)-min(lis)) for i in lis]\n",
    "normilation([1,2,34,67,290,23,56,78,60,100,7,8,9,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
