{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a class and object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.A object at 0x12defd990>\n"
     ]
    }
   ],
   "source": [
    "class A():\n",
    "    pass\n",
    "c = A()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first parameter of the method must be refered as self ,self point to the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello welcome\n"
     ]
    }
   ],
   "source": [
    "class Ed():\n",
    "    def fu(self):\n",
    "        print(\"hello welcome\")\n",
    "d = Ed()\n",
    "d.fu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two type of class attributes present\n",
    "- user defined class attributes\n",
    "- in built class attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In build class attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__module__': '__main__', 'fu': <function Ed.fu at 0x12d313050>, '__dict__': <attribute '__dict__' of 'Ed' objects>, '__weakref__': <attribute '__weakref__' of 'Ed' objects>, '__doc__': None}\n",
      "None\n",
      "The class name is :-  Ed\n",
      "the module name is :-  __main__\n",
      "This module gives the base class names : - <class 'object'>\n"
     ]
    }
   ],
   "source": [
    "print(Ed.__dict__)\n",
    "#it shows the class documentation\n",
    "print(Ed.__doc__)\n",
    "#This gives the class name\n",
    "print(\"The class name is :- \",Ed.__name__)\n",
    "#this gives the module name in which the class is defined\n",
    "print(\"the module name is :- \",Ed.__module__)\n",
    "#The belowe attribute tells about the base class\n",
    "print(\"This module gives the base class names : -\",Ed.__base__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user defined attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A():\n",
    "    def f():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some access specifier for the attributes\n",
    "- public\n",
    "- private\n",
    "- protected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The public variable is :-  hello i am public\n",
      "The protected variable is :-  hello i am protected\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'f' object has no attribute '__private'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-433-0ab40d9c70e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The public variable is :- \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The protected variable is :- \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Thr private variable is :- \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__private\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'f' object has no attribute '__private'"
     ]
    }
   ],
   "source": [
    "class f:\n",
    "    def __init__(self):\n",
    "        self.public = (\"hello i am public\")\n",
    "        self._protected = (\"hello i am protected\")\n",
    "        self.__private = (\"I am private\")\n",
    "b = f()\n",
    "print(\"The public variable is :- \",b.public)\n",
    "print(\"The protected variable is :- \",b._protected)\n",
    "print(\"Thr private variable is :- \",b.__private)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : we can have public ,private and potected methods as wll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class variable and instance variableB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "class G():\n",
    "    #class variable\n",
    "    name = \"hello\"\n",
    "    def some_name(self,name1):\n",
    "        #class variable\n",
    "        self.name1 = name1\n",
    "#instance variable\n",
    "obj = G()\n",
    "print(obj.name)\n",
    "obj.some_name(\"rohan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cconstructure and distructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructure is init\n",
      "destructing the object\n"
     ]
    }
   ],
   "source": [
    "class D():\n",
    "    def __init__(self):\n",
    "        print(\"constructure is init\")\n",
    "    def __del__(self):\n",
    "        print(\"destructing the object\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    b = D()\n",
    "    del b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiple constructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class D():\n",
    "    def __init__(self,name,roll_no,height,weight):\n",
    "        self.name = name\n",
    "        self.roll_no = roll_no\n",
    "        self.height = height\n",
    "        self.weight = weight\n",
    "    @classmethod\n",
    "    def r(self,roll_no,height,weight,name):\n",
    "        self.name = name\n",
    "        self.roll_no = roll_no\n",
    "        self.height = height\n",
    "        self.weight = weight\n",
    "v = D(\"rohan\",23,45,67)\n",
    "v.roll_no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inheretance\n",
    "single ingeretance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello i am a base class\n"
     ]
    }
   ],
   "source": [
    "class base():\n",
    "    def fu(self):\n",
    "        print(\"hello i am a base class\")\n",
    "class child(base):\n",
    "    def f(self):\n",
    "        print(\"hello i am a child class\")\n",
    "c = child()\n",
    "c.fu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multiple inheretance and super keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class base:\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "    pass\n",
    "class ch(base):\n",
    "    def __init__(self,name):\n",
    "        super(). __init__(name)\n",
    "obj = ch(\"hello\")\n",
    "obj.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating the data for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# create dummy data for training\n",
    "\n",
    "#creating the x_values\n",
    "x_values = [i for i in range(11)]\n",
    "#making the list to array\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "#reshaping the array\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "\n",
    "\n",
    "#creating the y_values \n",
    "y_values = [2*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 4.],\n",
       "       [ 5.],\n",
       "       [ 6.],\n",
       "       [ 7.],\n",
       "       [ 8.],\n",
       "       [ 9.],\n",
       "       [10.]], dtype=float32)"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating the linear regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "down below we are creating the neural network,where inputDim tells the number of input bubbles in the input layer,outputDim tells the number of output bubbles in the output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 1        # takes variable 'x' \n",
    "outputDim = 1       # takes variable 'y'\n",
    "learningRate = 0.01 \n",
    "epochs = 100\n",
    "\n",
    "#now the network is build.\n",
    "model = linearRegression(inputDim, outputDim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we are passing the data into neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we initialize the loss (Mean Squared Error) and optimization (Stochastic Gradient Descent) functions that we’ll use in the training of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing all the initializations, we can now begin to train our model. Following is the code for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(228.6360, grad_fn=<MseLossBackward>)\n",
      "epoch 0, loss 228.63597106933594\n",
      "tensor(18.6512, grad_fn=<MseLossBackward>)\n",
      "epoch 1, loss 18.651185989379883\n",
      "tensor(1.5234, grad_fn=<MseLossBackward>)\n",
      "epoch 2, loss 1.5233683586120605\n",
      "tensor(0.1263, grad_fn=<MseLossBackward>)\n",
      "epoch 3, loss 0.12628339231014252\n",
      "tensor(0.0123, grad_fn=<MseLossBackward>)\n",
      "epoch 4, loss 0.012304503470659256\n",
      "tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "epoch 5, loss 0.002985354047268629\n",
      "tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "epoch 6, loss 0.0022030582185834646\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "epoch 7, loss 0.002117373514920473\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "epoch 8, loss 0.00208874698728323\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "epoch 9, loss 0.0020650196820497513\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "epoch 10, loss 0.002041914965957403\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "epoch 11, loss 0.002019123872742057\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "epoch 12, loss 0.0019965735264122486\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "epoch 13, loss 0.0019742692820727825\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "epoch 14, loss 0.0019522157963365316\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "epoch 15, loss 0.0019304356537759304\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "epoch 16, loss 0.0019088729750365019\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "epoch 17, loss 0.001887557446025312\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "epoch 18, loss 0.0018664853414520621\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "epoch 19, loss 0.0018456469988450408\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "epoch 20, loss 0.001825035666115582\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "epoch 21, loss 0.0018046636832877994\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "epoch 22, loss 0.0017844941467046738\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "epoch 23, loss 0.0017645644256845117\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "epoch 24, loss 0.0017448844155296683\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "epoch 25, loss 0.0017253811238333583\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "epoch 26, loss 0.0017061165999621153\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "epoch 27, loss 0.0016870490508154035\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "epoch 28, loss 0.0016682096756994724\n",
      "tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "epoch 29, loss 0.0016495900927111506\n",
      "tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "epoch 30, loss 0.0016311734216287732\n",
      "tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "epoch 31, loss 0.0016129682771861553\n",
      "tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "epoch 32, loss 0.001594939036294818\n",
      "tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "epoch 33, loss 0.0015771142207086086\n",
      "tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "epoch 34, loss 0.0015595384174957871\n",
      "tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "epoch 35, loss 0.0015420869458466768\n",
      "tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "epoch 36, loss 0.0015248872805386782\n",
      "tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "epoch 37, loss 0.0015078395372256637\n",
      "tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "epoch 38, loss 0.001491032657213509\n",
      "tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "epoch 39, loss 0.0014743647770956159\n",
      "tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "epoch 40, loss 0.0014579236740246415\n",
      "tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "epoch 41, loss 0.0014416129561141133\n",
      "tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "epoch 42, loss 0.0014255259884521365\n",
      "tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "epoch 43, loss 0.0014096120139583945\n",
      "tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "epoch 44, loss 0.0013938500778749585\n",
      "tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "epoch 45, loss 0.0013782897731289268\n",
      "tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "epoch 46, loss 0.0013629008317366242\n",
      "tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "epoch 47, loss 0.00134768127463758\n",
      "tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "epoch 48, loss 0.0013326512416824698\n",
      "tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "epoch 49, loss 0.0013177525252103806\n",
      "tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "epoch 50, loss 0.0013030244736000896\n",
      "tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "epoch 51, loss 0.0012885082978755236\n",
      "tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "epoch 52, loss 0.0012741140089929104\n",
      "tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "epoch 53, loss 0.001259878627024591\n",
      "tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "epoch 54, loss 0.0012458133278414607\n",
      "tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "epoch 55, loss 0.001231878180988133\n",
      "tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "epoch 56, loss 0.0012181394267827272\n",
      "tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "epoch 57, loss 0.001204540254548192\n",
      "tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "epoch 58, loss 0.0011910699540749192\n",
      "tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "epoch 59, loss 0.001177778816781938\n",
      "tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "epoch 60, loss 0.001164617482572794\n",
      "tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "epoch 61, loss 0.0011516246013343334\n",
      "tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "epoch 62, loss 0.0011387595441192389\n",
      "tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "epoch 63, loss 0.0011260502506047487\n",
      "tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "epoch 64, loss 0.0011134776286780834\n",
      "tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "epoch 65, loss 0.001101041561923921\n",
      "tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "epoch 66, loss 0.0010887429816648364\n",
      "tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "epoch 67, loss 0.0010765717597678304\n",
      "tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "epoch 68, loss 0.0010645772563293576\n",
      "tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "epoch 69, loss 0.0010526779806241393\n",
      "tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "epoch 70, loss 0.001040926668792963\n",
      "tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "epoch 71, loss 0.0010292981751263142\n",
      "tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "epoch 72, loss 0.0010177972726523876\n",
      "tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "epoch 73, loss 0.0010064539965242147\n",
      "tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "epoch 74, loss 0.0009952026885002851\n",
      "tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "epoch 75, loss 0.0009840906132012606\n",
      "tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "epoch 76, loss 0.0009730944293551147\n",
      "tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "epoch 77, loss 0.0009622379438951612\n",
      "tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "epoch 78, loss 0.0009514887351542711\n",
      "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "epoch 79, loss 0.0009408561163581908\n",
      "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "epoch 80, loss 0.0009303588303737342\n",
      "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "epoch 81, loss 0.0009199650958180428\n",
      "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "epoch 82, loss 0.0009096948197111487\n",
      "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "epoch 83, loss 0.0008995202952064574\n",
      "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "epoch 84, loss 0.0008894934435375035\n",
      "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "epoch 85, loss 0.0008795628091320395\n",
      "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "epoch 86, loss 0.0008697452722117305\n",
      "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "epoch 87, loss 0.0008600315195508301\n",
      "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "epoch 88, loss 0.0008504342986270785\n",
      "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "epoch 89, loss 0.0008409332949668169\n",
      "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "epoch 90, loss 0.0008315405575558543\n",
      "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "epoch 91, loss 0.0008222405449487269\n",
      "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "epoch 92, loss 0.0008130705100484192\n",
      "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "epoch 93, loss 0.0008039838867262006\n",
      "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "epoch 94, loss 0.0007950106519274414\n",
      "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "epoch 95, loss 0.0007861295598559082\n",
      "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "epoch 96, loss 0.0007773541146889329\n",
      "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "epoch 97, loss 0.0007686791359446943\n",
      "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "epoch 98, loss 0.0007600842509418726\n",
      "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "epoch 99, loss 0.0007516042096540332\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Converting inputs and labels to Variable\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our Linear Regression Model is trained, let’s test it. Since it’s a very trivial model, we’ll test this on our existing dataset and also plot to see the original vs the predicted outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9490019]\n",
      " [ 2.956346 ]\n",
      " [ 4.9636903]\n",
      " [ 6.9710345]\n",
      " [ 8.978379 ]\n",
      " [10.9857235]\n",
      " [12.993068 ]\n",
      " [15.000412 ]\n",
      " [17.007755 ]\n",
      " [19.015099 ]\n",
      " [21.022444 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    if torch.cuda.is_available():\n",
    "        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
    "    else:\n",
    "        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "    print(predicted)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
    "plt.plot(x_train, predicted, '--', label='Predictions', alpha=0.5)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12,  3])"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([12,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor([[1,2,3],[4,56,6]])\n",
    "y=torch.tensor([[78,89,45],[90,89,54]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appling ANN on the vector data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note :- If we have the regression problem ,then we should not apply activation function,if its a classification problem ,then we apply activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preperation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# create dummy data for training\n",
    "\n",
    "#creating the x_values\n",
    "x_values = [i for i in range(11)]\n",
    "#making the list to array\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "#reshaping the array\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "\n",
    "\n",
    "#creating the y_values \n",
    "y_values = [2*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the neural network frame work.\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(inputSize, outputSize)\n",
    "        self.layer2 = torch.nn.Linear(outputSize,outputSize)\n",
    "    def forward(self, x):\n",
    "        layer1_output = self.layer1(x)\n",
    "        layer2_output = self.layer2(layer1_output)\n",
    "        return layer2_output\n",
    "        #return layer1_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linearRegression(\n",
      "  (layer1): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (layer2): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#initilizing the neural network\n",
    "brain = linearRegression(1,1)\n",
    "print(brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "#few parameter for the running the model.\n",
    "learning_rate = 0.01\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3) Optimizer and Loss\n",
    "Next, you should define the Optimizer and the Loss Function for our training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer and Loss Function\n",
    "optimizer = torch.optim.SGD(brain.parameters(), lr=learning_rate)\n",
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4) Training\n",
    "\n",
    "Now let's start our training process. With an epoch of 250, you will iterate our data to find the best value for our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=Variable(torch.from_numpy(x_train),requires_grad=True)\n",
    "y=Variable(torch.from_numpy(y_train),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 138.03195190429688 \n",
      "epoch 1, loss 13.34854793548584 \n",
      "epoch 2, loss 40.27155685424805 \n",
      "epoch 3, loss 141.5234832763672 \n",
      "epoch 4, loss 38.470733642578125 \n",
      "epoch 5, loss 53.58150863647461 \n",
      "epoch 6, loss 156.42030334472656 \n",
      "epoch 7, loss 113.56758117675781 \n",
      "epoch 8, loss 5.216117858886719 \n",
      "epoch 9, loss 16.254133224487305 \n",
      "epoch 10, loss 64.8109130859375 \n",
      "epoch 11, loss 23.054851531982422 \n",
      "epoch 12, loss 86.7938461303711 \n",
      "epoch 13, loss 3.3117876052856445 \n",
      "epoch 14, loss 12.900925636291504 \n",
      "epoch 15, loss 31.247724533081055 \n",
      "epoch 16, loss 108.26042938232422 \n",
      "epoch 17, loss 5.051312446594238 \n",
      "epoch 18, loss 14.602633476257324 \n",
      "epoch 19, loss 56.136112213134766 \n",
      "epoch 20, loss 25.649555206298828 \n",
      "epoch 21, loss 90.85212707519531 \n",
      "epoch 22, loss 0.44618624448776245 \n",
      "epoch 23, loss 0.7807616591453552 \n",
      "epoch 24, loss 1.931172490119934 \n",
      "epoch 25, loss 6.636483669281006 \n",
      "epoch 26, loss 17.766050338745117 \n",
      "epoch 27, loss 65.75749206542969 \n",
      "epoch 28, loss 14.220378875732422 \n",
      "epoch 29, loss 53.04012680053711 \n",
      "epoch 30, loss 24.508569717407227 \n",
      "epoch 31, loss 85.10555267333984 \n",
      "epoch 32, loss 0.9174984097480774 \n",
      "epoch 33, loss 2.398853063583374 \n",
      "epoch 34, loss 6.618032932281494 \n",
      "epoch 35, loss 24.318714141845703 \n",
      "epoch 36, loss 35.35862731933594 \n",
      "epoch 37, loss 110.3955078125 \n",
      "epoch 38, loss 17.17125129699707 \n",
      "epoch 39, loss 29.95075035095215 \n",
      "epoch 40, loss 96.04425811767578 \n",
      "epoch 41, loss 3.6635382175445557 \n",
      "epoch 42, loss 9.208412170410156 \n",
      "epoch 43, loss 32.32748794555664 \n",
      "epoch 44, loss 30.914440155029297 \n",
      "epoch 45, loss 96.47761535644531 \n",
      "epoch 46, loss 5.537357807159424 \n",
      "epoch 47, loss 12.850276947021484 \n",
      "epoch 48, loss 43.94932556152344 \n",
      "epoch 49, loss 22.506444931030273 \n",
      "epoch 50, loss 73.17369842529297 \n",
      "epoch 51, loss 2.1303205490112305 \n",
      "epoch 52, loss 5.999044895172119 \n",
      "epoch 53, loss 13.365839004516602 \n",
      "epoch 54, loss 44.905147552490234 \n",
      "epoch 55, loss 20.214340209960938 \n",
      "epoch 56, loss 65.58685302734375 \n",
      "epoch 57, loss 4.939755439758301 \n",
      "epoch 58, loss 15.660327911376953 \n",
      "epoch 59, loss 24.524158477783203 \n",
      "epoch 60, loss 76.65485382080078 \n",
      "epoch 61, loss 0.6448674201965332 \n",
      "epoch 62, loss 0.8401756882667542 \n",
      "epoch 63, loss 1.4150116443634033 \n",
      "epoch 64, loss 3.4441566467285156 \n",
      "epoch 65, loss 7.835045337677002 \n",
      "epoch 66, loss 25.56580924987793 \n",
      "epoch 67, loss 26.695253372192383 \n",
      "epoch 68, loss 81.39691162109375 \n",
      "epoch 69, loss 0.9215436577796936 \n",
      "epoch 70, loss 1.6342238187789917 \n",
      "epoch 71, loss 4.136447429656982 \n",
      "epoch 72, loss 9.091439247131348 \n",
      "epoch 73, loss 29.400129318237305 \n",
      "epoch 74, loss 24.88031005859375 \n",
      "epoch 75, loss 75.81057739257812 \n",
      "epoch 76, loss 0.5106651782989502 \n",
      "epoch 77, loss 0.4994114935398102 \n",
      "epoch 78, loss 0.4982239603996277 \n",
      "epoch 79, loss 0.526918888092041 \n",
      "epoch 80, loss 0.6365034580230713 \n",
      "epoch 81, loss 1.0151273012161255 \n",
      "epoch 82, loss 2.016042709350586 \n",
      "epoch 83, loss 5.560888290405273 \n",
      "epoch 84, loss 11.670258522033691 \n",
      "epoch 85, loss 37.71627426147461 \n",
      "epoch 86, loss 20.431303024291992 \n",
      "epoch 87, loss 63.561710357666016 \n",
      "epoch 88, loss 3.421790838241577 \n",
      "epoch 89, loss 10.084000587463379 \n",
      "epoch 90, loss 17.540843963623047 \n",
      "epoch 91, loss 54.829586029052734 \n",
      "epoch 92, loss 7.802517890930176 \n",
      "epoch 93, loss 24.38818359375 \n",
      "epoch 94, loss 23.360881805419922 \n",
      "epoch 95, loss 69.94237518310547 \n",
      "epoch 96, loss 0.7137946486473083 \n",
      "epoch 97, loss 1.2044588327407837 \n",
      "epoch 98, loss 2.387991189956665 \n",
      "epoch 99, loss 6.521340847015381 \n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    prediction = brain(x)\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_func(prediction,y)\n",
    "    loss.backward()        \n",
    "    optimizer.step()\n",
    "    print('epoch {}, loss {} '.format(i, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3154],\n",
       "        [ 4.7117],\n",
       "        [ 7.1079],\n",
       "        [ 9.5042],\n",
       "        [11.9005],\n",
       "        [14.2967],\n",
       "        [16.6930],\n",
       "        [19.0892],\n",
       "        [21.4855],\n",
       "        [23.8818],\n",
       "        [26.2780]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre=brain(x)\n",
    "pre=pre.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error is :-  3.5268320449627972\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rms = sqrt(mean_squared_error(y.detach().numpy(),pre))\n",
    "print(\"The error is :- \",rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
    "plt.plot(x_train, pre, '--', label='Predictions', alpha=0.5)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a new value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14.2967]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain(Variable(torch.tensor([[float(5)]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([i for i in range(0,12)],dtype = np.float32)\n",
    "y_train = np.array([0,1,0,1,0,1,0,1,0,1,0,1],dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 1)\n",
    "y_train = y_train.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the structure of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self,inputSize, outputSize):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        #As logistic regression use sigmoid function,so we are using sigmoid as our activation function.\n",
    "        output = self.linear(x)\n",
    "        y_pred = self.sigmoid(output)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating the model of the above neural network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "#few parameter for the running the model.\n",
    "learning_rate = 0.01\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the loss function, we use Binary Cross-Entropy (BCE), which is known as the binary logarithmic loss function. This is commonly used for logistic regression tasks since we are predicting a binary value as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer and Loss Function\n",
    "optimizer = torch.optim.SGD(brain.parameters(), lr=learning_rate)\n",
    "#loss_func = torch.nn.MSELoss() for regression\n",
    "loss_func = torch.nn.BCELoss(size_average=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=Variable(torch.from_numpy(x_train),requires_grad=True)\n",
    "y=Variable(torch.from_numpy(y_train),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]], requires_grad=True)"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "the derivative for 'target' is not implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-546-a6cd233ae6b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2064\u001b[0m     return torch._C._nn.binary_cross_entropy(\n\u001b[0;32m-> 2065\u001b[0;31m         input, target, weight, reduction_enum)\n\u001b[0m\u001b[1;32m   2066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: the derivative for 'target' is not implemented"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    model.train()\n",
    "    prediction = model(x)\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_func(prediction,y)\n",
    "    loss.backward()        \n",
    "    optimizer.step()\n",
    "    print('epoch {}, loss {} '.format(i, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually building weights and biases\n",
    "One way to approach this is by building all the blocks. This is a low level approach, but it may be suited if you’re trying to reproduce the latest and greatest deep learning architecture on a paper you just just read. Or maybe if you want to develop a customized layer. Either way, PyTorch has you covered. You’ll need to define your weights and biases, but if you’re comfortable at that level, you’re good to go. The key thing here is that you will need to tell PyTorch what is variable or optimizable in your network, so that PyTorch knows how to perform gradient descent on your network. Let’s look at how someone might approach this in low level PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# generating some random features\n",
    "features = torch.randn(1, 16) \n",
    "\n",
    "# define the weights\n",
    "W1 = torch.randn((16, 12), requires_grad=True)\n",
    "W2 = torch.randn((12, 10), requires_grad=True)\n",
    "W3 = torch.randn((10, 1), requires_grad=True)\n",
    "\n",
    "# define the bias terms\n",
    "B1 = torch.randn((12), requires_grad=True)\n",
    "B2 = torch.randn((10), requires_grad=True)\n",
    "B3 = torch.randn((1), requires_grad=True)\n",
    "\n",
    "# calculate hidden and output layers\n",
    "h1 = F.relu((features @ W1) + B1)\n",
    "h2 = F.relu((h1 @ W2) + B2)\n",
    "output = torch.sigmoid((h2 @ W3) + B3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extending the torch.nn.Model Class\n",
    "In practice, most of us will likely use predefined layers and activation functions to train our networks. There are a couple of routes to go if you’re headed in this direction. A more elegant approach involves creating your own neural network python class, by extending the Model class from torch.nn. There are many advantages of defining a neural network this way and perhaps most notably, it allows one to inherit all of the functionality of the torch.nn module while allowing the flexibility of overwriting the default model construction and forward pass method. In this approach, we will define two methods:\n",
    "1. The class constructor, __init__\n",
    "2. The forward method\n",
    "The first is the initializer of the class and is where you’ll define the layers that will compose the network. Typically we don’t need to define the activation functions here since they can be defined in the forward pass (i.e. in the forward method), but it’s not a rule and you can certainly do that if you want to (we’ll actually see an example at the end).\n",
    "The second method is where you define the forward pass. This method takes an input that represents the features the model will be trained on. Here, you can call the activation functions and pass in as parameters the layers you’ve previously defined in the constructor method. You’ll need to pass the input as an argument to the first layer and after processing the activations, that output can be fed into the next layer and so on.\n",
    "Let’s take a look at how we could do this in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNetwork(\n",
      "  (fc1): Linear(in_features=16, out_features=12, bias=True)\n",
      "  (fc2): Linear(in_features=12, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "# define the network class\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        # call constructor from superclass\n",
    "        super().__init__()\n",
    "        \n",
    "        # define network layers\n",
    "        self.fc1 = nn.Linear(16, 12)\n",
    "        self.fc2 = nn.Linear(12, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # define forward pass\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# instantiate the model\n",
    "model = MyNetwork()\n",
    "\n",
    "# print model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Using torch.nn.Sequential\n",
    "There is still a more compact way to define neural networks in pytorch. This is a modular approach, made possible by the torch.nn.Sequential module and is especially appealing if you come from a Keras background, where you can define sequential layers, kind of like building something from lego blocks. This is a very similar approach to Keras’s sequential API and leverages the torch.nn pre-built layers and activation functions. Using this approach, our feed-forward network can be defined a follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=16, out_features=12, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=12, out_features=10, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (5): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# define model architecture\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(16, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# print model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
